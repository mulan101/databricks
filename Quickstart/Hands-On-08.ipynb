{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8962229b-1315-4080-bb50-38922536dcae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show external locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48143ee-3afc-4c2c-b7fd-f85b871fba57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "external_location = \"s3://databricks-data-bronze/example01\"\n",
    "catalog = \"hjh_workspace\"\n",
    "\n",
    "dbutils.fs.put(f\"{external_location}/filename.txt\", \"Hello world!\", True)\n",
    "display(dbutils.fs.head(f\"{external_location}/filename.txt\"))\n",
    "dbutils.fs.rm(f\"{external_location}/filename.txt\")\n",
    "\n",
    "display(spark.sql(f\"SHOW SCHEMAS IN {catalog}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e811b98f-8a48-4451-aa09-e80896eeef26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set parameters for isolation in workspace and reset demo\n",
    "username = spark.sql(\"SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')\").first()[0]\n",
    "database = f\"{catalog}.e2e_lakehouse_{username}_db\"\n",
    "source = f\"{external_location}/e2e-lakehouse-source\"\n",
    "table = f\"{database}.target_table\"\n",
    "checkpoint_path = f\"{external_location}/_checkpoint/e2e-lakehouse-demo\"\n",
    "\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {database} CASCADE\")\n",
    "spark.sql(f\"CREATE DATABASE {database}\")\n",
    "spark.sql(f\"USE {database}\")\n",
    "\n",
    "# Clear out data from previous demo execution\n",
    "dbutils.fs.rm(source, True)\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "# Define a class to load batches of data to source\n",
    "class LoadData:\n",
    "\n",
    "    def __init__(self, source):\n",
    "        self.source = source\n",
    "\n",
    "    def get_date(self):\n",
    "        try:\n",
    "            df = spark.read.format(\"json\").load(source)\n",
    "            df.count()\n",
    "        except:\n",
    "            return \"2016-01-01\" \n",
    "        batch_date = df.selectExpr(\"max(distinct(date(tpep_pickup_datetime))) + 1 day\").first()[0]\n",
    "        if batch_date.month == 3:\n",
    "            raise Exception(\"Source data exhausted\")\n",
    "        return batch_date\n",
    "\n",
    "    def get_batch(self, batch_date):\n",
    "        return (\n",
    "            spark.table(\"samples.nyctaxi.trips\")\n",
    "            .filter(col(\"tpep_pickup_datetime\").cast(\"date\") == batch_date)\n",
    "        )\n",
    "\n",
    "    def write_batch(self, batch):\n",
    "        batch.write.format(\"json\").mode(\"append\").save(self.source)\n",
    "\n",
    "    def land_batch(self):\n",
    "        batch_date = self.get_date()\n",
    "        batch = self.get_batch(batch_date)\n",
    "        self.write_batch(batch)\n",
    "\n",
    "RawData = LoadData(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7e46ca-34c9-4c13-83e7-3493bbb22f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RawData.land_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcb3484-d67b-496a-a73f-99de63d7c2b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# Configure Auto Loader to ingest JSON data to a Delta table\n",
    "(spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"json\")\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "  .load(source)\n",
    "  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"mergeSchema\", \"true\")\n",
    "  .toTable(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfaf328d-2a3e-4478-a62a-47bd9ecbac39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(table)\n",
    "display(df.groupBy(\"processing_time\").count().withColumnRenamed(\"count\", \"records\").orderBy(\"processing_time\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6201ad7-529c-4bd2-bc11-338234577ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5585756137402474,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hands-On-08",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
